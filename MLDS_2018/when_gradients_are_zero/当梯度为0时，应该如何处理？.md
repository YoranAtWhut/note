# 当梯度为0时，应该如何处理？
###[`课程链接`](https://www.bilibili.com/video/av24015685/?p=4)

------

&#160;&#160;&#160;&#160;&#160;&#160;引言：当我们训练神经网络的时候，常会遇到梯度为零的情况，此时训练将无法继续下去。在这种情况下人们可能会认为参数θ已经到达了local minima，然而事实可能并非这样。梯度为0，只能说明θ到达了一个critical point，这个critical point可能是局部极小值，也可能是局部最大值，也可能是鞍点。那么当梯度为零的时候，我们该如何处理呢？

&#160;&#160;&#160;&#160;&#160;&#160;对于神经网络训练过程中要优化的损失函数f(θ)，可以分解为$f(θ)=f(\theta^0)+(\theta-\theta^0)^Tg+{1\over2}(\theta-\theta^0)^TH(\theta-\theta^0)+…$，其中
$$g是梯度，它是一个向量，g_i={\partial f(\theta^0)\over\partial \theta_i}，指的是f(\theta)在\theta^0处的微分。$$
$$H是hessian矩阵，H_{ij}={\partial^2f(\theta^0)\over\partial\theta_i\partial\theta_j}$$
&#160;&#160;&#160;&#160;&#160;&#160;这里就是用右边的那些项来描述左边的f(θ)，我个人感觉好像有点像泰勒级数。。。

&#160;&#160;&#160;&#160;&#160;&#160;上面说到右边的三项是用来模拟f(θ)的，其中，第一项是一条水平直线，第二项是一条有斜率的线（一阶），第三项则是一条二次曲线（二阶）。因此，可以认为H决定了曲线的弯曲方向，如下图所示：

![](https://github.com/YoranAtWhut/note/blob/master/MLDS_2018/when_gradients_are_zero/pic2.png)

&#160;&#160;&#160;&#160;&#160;&#160;上图中，绿色虚线则仅考虑了前两项：$f(θ^0)$和$(\theta-\theta^0)^Tg$。

&#160;&#160;&#160;&#160;&#160;&#160;通过上述公式，我们可以找到一种新的优化方法，即牛顿法：忽略等式右边第三项之后的项，对上面的等式取偏微分，并令偏微分结果为0，即可得到梯度为零的点。（注意，这里得到梯度为0的点，并非是实际的f(θ)梯度为0的点，而是级数所模拟的函数梯度为0的点）具体计算过程如下所示：

![](https://github.com/YoranAtWhut/note/blob/master/MLDS_2018/when_gradients_are_zero/pic3.png)

&#160;&#160;&#160;&#160;&#160;&#160;那么牛顿法的具体做法就是：首先随机找一个点$\theta^0$，计算二阶级数在$\theta^0$模拟出的函数梯度为0的点，找到这个点之后，将其作$\theta^1$,接着再计算二阶级数在$\theta^1$模拟出的函数梯度为0的点，即为$\theta^2$，以此类推，最终得到一个点就是真正的f(θ)梯度为0的点。而如果f(θ)就是二次的，牛顿法一次就能找到梯度为0的点。牛顿法的迭代过程就如下图所示（下图中x就是θ）：

![](https://github.com/YoranAtWhut/note/blob/master/MLDS_2018/when_gradients_are_zero/pic4.png)

&#160;&#160;&#160;&#160;&#160;&#160;但是深度学习中并不适用牛顿法来优化参数，主要原因有两个，一是因为实际中H的维度过大，第二则是因为牛顿法只保证找到梯度为0的地方，这个地方可能是local minima、local maxim甚至是saddle point。

&#160;&#160;&#160;&#160;&#160;&#160;现在假设经过多轮的参数调整，已经到了一个critical point（也就是gradient=0的点），那么上面那个等式中，起决定因素的就是第三项了，此时我们可以通过H矩阵来判断当前点到底是什么形状的。

![](https://github.com/YoranAtWhut/note/blob/master/MLDS_2018/when_gradients_are_zero/pic5.png)

&#160;&#160;&#160;&#160;&#160;&#160;当到了critical point的时候，上面的等式就变成了$f(θ)=f(\theta^0)+{1\over2}(\theta-\theta^0)^TH(\theta-\theta^0)+…$，此时我们可以将$(\theta-\theta^0)$看成是x，此时就可以根据H是何种类型的矩阵来判断当前critical point的形状了（H可能的类型是正定矩阵（特征值全部大于0）、半正定（特征值大于等于0）、负定（特征值全部小于0）、半负定（特征值小于等于0））。
&#160;&#160;&#160;&#160;&#160;&#160;假设H是正定的，那么f(θ)在$\theta^0$的领域内的值都是大于$f(\theta^0)$的，此时$\theta^0$就是一个局部最小值。如果H是负定的，那么$\theta^0$就是一个局部最大值。如果H是半正定或者半负定的，那么$\theta^0$是无法确定的，因为f(θ)可能大于$f(\theta^0)$，也可能小于，因为当$(\theta-\theta^0)^TH(\theta-\theta^0)$等于零的时候，此时决定f(θ)值的就是后续的项了。

&#160;&#160;&#160;&#160;&#160;&#160;当$(\theta-\theta^0)$等于H的某个单位特征向量$v$的时候，此时$(\theta-\theta^0)^TH(\theta-\theta^0)=\lambda$，也就是说，当θ从$\theta^0$移动$v$，则f(θ)相应增加对应的特征值（忽略了前面的系数1/2）。如下图所示：

![](https://github.com/YoranAtWhut/note/blob/master/MLDS_2018/when_gradients_are_zero/pic6.png)

&#160;&#160;&#160;&#160;&#160;&#160;而当$(\theta-\theta^0)=a_1v_1+a_2v_2+...+a_nv_n$的时候(θ空间中，任何一个向量都可以如此表示，因为H是一个对称矩阵，所以它的特征向量是一个基)，f(θ)相应增加$a_1^2\lambda_1+a_2^2\lambda_2+...+a_n^2\lambda_n$，如果某些特征值是正的，某些特征值是负的，也就是说在当前点往某些方向走是正的，某些方向走是负的，那么这个点就是鞍点。


&#160;&#160;&#160;&#160;&#160;&#160;最后再说一下，当某次training的error不变以后，此时的gradient并不一定等于零，也就是说此时参数并没有到达某个critical point，如下图是某个人做的实验，当error rate不变了之后，此时gradient的范数并不等于零：

![](https://github.com/YoranAtWhut/note/blob/master/MLDS_2018/when_gradients_are_zero/pic7.png)

&#160;&#160;&#160;&#160;&#160;&#160;这是另一个人的实验，可以看出，gradient会在某个过程逐渐变小，这是参数逐渐靠近某个鞍点，然后又逃脱出鞍点的范围，此时gradient又突然变大：
![](https://github.com/YoranAtWhut/note/blob/master/MLDS_2018/when_gradients_are_zero/pic8.png)